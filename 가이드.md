# <데이터 전처리>

### 1. 데이터 불러오기
```py
train = pd.read_csv('')
test = pd.read_csv('')
submit = pd.read_csv('')
```
### 2. 데이터 슬라이싱 (데이터 확인 후 불필요한 변수 제거 및 타겟 변수 분리)
```py
train_x = train.drop(['index', 'target'], axis=1)
train_y = train['target']
test = test.drop(['index'], axis=1)
```
### 3. 데이터 실수화
```py
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
train_y = le.fit_transform(train_y)
# 실수화 다시 풀기
pred = le.inverse_transform(pred)
```

### 4. 데이터 표준화
```py
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
scaler = StandardScaler()
scaler.fit(train_x)
train_x = scaler.transform(train_x)
test = scaler.transform(test)
```

### 5. 범주 나누기
```py
train_x['age_1'] = 0
train_x.loc[train_x['age']<=20, 'age_1'] = 0
train_x.loc[train_x['age']>20, 'age_1'] = 1
train_x = train_x.drop('age', axis=1)
```

### 6. NaN값 제거

---

# <모델 - Auto ML>
```py
# install autoskearn
!apt-get install build-essential swig
!pip install auto-sklearn

!apt-get remove swig
!apt-get install swig3.0 build-essential -y
!ln -s /usr/bin/swig3.0 /usr/bin/swig
!pip install --upgrade setuptools
```
### 1. Auto ML (분류)
```py
import autosklearn.classification
from joblib import dump, load
automl = autosklearn.classification.AutoSklearnClassifier(
                                                           time_left_for_this_task=120,
                                                           per_run_time_limit=1000,    #수정가능
                                                           include_estimators=["k_nearest_neighbors",  "random_forest", 'decision_tree', 
                                                                                'libsvm_svc', 'adaboost', 'gradient_boosting', 'lda', 'qda'],
                                                            n_jobs=-1, ensemble_size = 3, seed = 1   #수정가능
                                                          )
automl.fit(train_x, train_y)
pred = automl.predict(test)
dump(automl, 'automl.joblib')
submit['target'] = pred
submit.to_csv('submit_automl.csv', index=False)
print(automl.show_models())
```

### 2. Auto ML (회귀)
```py
import autosklearn.regression
from joblib import dump, load
automl = autosklearn.regression.AutoSklearnRegressor(
                                                     time_left_for_this_task=120,
                                                     per_run_time_limit=1000,     #수정가능
                                                     include_estimators=["k_nearest_neighbors",  "random_forest", 'decision_tree', 
                                                                         'libsvm_svr', 'adaboost', 'gradient_boosting'],
                                                     n_jobs=-1, ensemble_size = 3, seed = 1   #수정가능
                                                    )
automl.fit(train_x, train_y)
pred = automl.predict(test)
dump(automl, 'automl.joblib')
submit['target'] = pred
submit.to_csv('submit_automl.csv', index=False)
print(automl.show_models())
```
---

# <모델 - 배운 것들>
### 1. KNN (분류 & 회귀)
```py
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=5, p=2)

from sklearn.neighbors import KNeighborsRegressor
model = KNeighborsRegressor(n_neighbors=5, weights= "distance") 
```
### 2. 로지스틱회귀 (분류)
```py
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(C=200, random_state=11) 
```
### 3. LDA & QDA (분류)
```py
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
model = LinearDiscriminantAnalysis(store_covariance=True, n_components=2)

from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
model = QuadraticDiscriminantAnalysis(store_covariance=True)
```
### 4. 의사결정나무 (분류 & 회귀)
```py
from sklearn.tree import DecisionTreeClassifier  
model = DecisionTreeClassifier()

from sklearn.tree import DecisionTreeRegressor
model = DecisionTreeRegressor()
```
### 5. SVM (분류 & 회귀)
```py
from sklearn.svm import SVC
model = SVC(kernel = ‘rbf’, C=10, gamma=“auto”)

from sklearn.svm import SVR
model = SVR(kernel = ‘rbf’, C=10, gamma=“auto”)
```

### 6. 앙상블 (voting)
```py
from sklearn.ensemble import VotingClassifier
logistic = LosisticRegression()
tree = DecisionTreeClassifier()
knn = KNeighborsClassifier()

Voting_estimators = [(‘logistic’, logistic), (‘tree’, tree), (‘knn’, knn)]
Voting = VotingClassifier(estimators = Voting_estimators, voting=‘soft’)

from sklearn.model_selection import GridSearchCV
Params = {‘logistic__C’ : [0.1, 1, 100], ‘tree__max_depth’:[1,3,5], ‘knn__n_neighbors’:[1,3,5]}
gcv = GridSearchCV(estimator=voting, param_grid = Params, cv=10, scoring=‘roc_auc’, iid=False)
gcv.fit(x_train, y_train)
print('final params', gcv.best_params_)
print('best score', gcv.best_score_)
```

### 7. 앙상블 (Bagging)
```py
from sklearn.ensemble import BaggingClassifier 

bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=None, criterion='entropy', random_state=1),
                            n_estimators=500,  
                            max_samples=1.0, 
                            max_features=1.0, 
                            bootstrap=True, 
                            bootstrap_features=False, 
                            n_jobs=1, 
                            random_state=1)
```
### 8. 앙상블 (Boosting)
```py
from sklearn.ensemble import AdaBoostClassifier 

adaboost = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1, criterion='entropy', random_state=1),
                              n_estimators=500,
                              learning_rate = 0.1, # 수정
                              random_state=1)
```

### 9. GBM, LightGBM
```py
from sklearn.ensemble import GradientBoostingClassifier
model = GradientBoostingClassifier(n_estimators=40)

from lightgbm import LGBMClassifier
model = LGBMClassifier(n_estimators=40)
```
---

# <모델 학습 및 예측값 저장>
```py
model.fit(train_x, train_y)
pred = model.pedict(test)
submit["target"] = pred
submit.to_csv('submit.csv', index=False)
```
---

# <기타>
### 1. 그리드 서치
```py
from sklearn.model_selection import GridSearchCV
model = svc(kernel=‘rbf’, class_weight=‘balanced’) 
param = {‘C’:[1,10,100,1000], ‘gamma’:[0.001,0.01,0.1]}
gcv = GridSearchCV(model, param, cv=10, n_jobs= -1)
gcv.fit()
print('final params', gcv.best_params_)
print('best score', gcv.best_score_)
```
### 2. KMeans 군집화
```py
from sklearn.cluster import KMeans
Kmeans = Kmeans(n_clusters=5, random_state=0)
Kmeans.fit(X)
Centers = Kmeans.cluster_centers_
```
### 4. 차원 축소 PCA
```py
from sklearn.decomposition import PCA
pca = PCA(n_components=10)
train_x_pca = pca.fit_transform(train_x)
test_pca = pca.transform(test)
```

### 5. 파이프라인
```py
from sklearn.pipeline import make_pipeline
pipeline = make_pipeline(StandardScaler(), PCA(), LogisticRegression())
pipeline.fit(train_x,train_y)
pred = pipeline.predict(test)

from sklearn.model_selection import cross_validate, GridSearchCV
scores = cross_validate(pipeline, train_x, train_y, cv = 10, return_train_sore=True)
gcv = GridSearchCV(pipeline, paraneters={}, scoring=‘accuracy’, cv = 10)
gcv.fit(x_train, y_train)
best_model = gcv.best_estimator_
pred = best_model.predict(test)
```


