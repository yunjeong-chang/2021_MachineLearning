# <데이터 전처리>

### 1. 데이터 불러오기
```py
train = pd.read_csv('')
test = pd.read_csv('')
submit = pd.read_csv('')
```
### 2. 데이터 슬라이싱 (데이터 확인 후 불필요한 변수 제거 및 타겟 변수 분리)
```py
train_x = train.drop(['index', 'target'], axis=1)
train_y = train['target']
test = test.drop(['index'], axis=1)
```
### 3. 데이터 실수화
```py
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
train_y = le.fit_transform(train_y)
# 실수화 다시 풀기
pred = le.inverse_transform(pred)
```
```py
from sklearn.preprocessing import LabelEncoder
columns = ['gender', 'Partner', 'Dependents', 'tenure', 
           'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity',
           'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',
           'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']

# 좀 더 일반적인 코드를 위해 train_x와 test를 합쳐서 label encoding 진행
le_input = pd.concat([train_x, test],  axis=0)
for column in columns:
  le = LabelEncoder()
  le.fit(le_input[column].values)
  train_x[column] = le.transform(train_x[column].values)
  test[column] = le.transform(test[column].values)
```

### 4. 데이터 표준화
```py
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
scaler = StandardScaler()
scaler.fit(train_x)
train_x = scaler.transform(train_x)
test = scaler.transform(test)
```

### 5. 범주 나누기
```py
train_x['age_1'] = 0
train_x.loc[train_x['age']<=20, 'age_1'] = 0
train_x.loc[train_x['age']>20, 'age_1'] = 1
train_x = train_x.drop('age', axis=1)
```

### 6. 데이터 타입 변경
```py
#object형이 끼어 있을 때 에러 방지를 위해 변경해줌
train_x = train_x.astype('float32')
test = test.astype('float32')

#최종 예측값을 int형으로 변경해서 넣어줘야 할 때
submit['target'] = pred.astype('int')
```

### 7. 숫자와 문자가 같이 섞여 있을 때 처리
```py
print(np.unique(train_x['ca'])) # 무슨 문자가 섞여있는지 확인하기
train_x[train_x['ca']=='?'] #어떤 행에 있는지 확인

#SimpleImputer 쓰기 위해 넘파이 배열로 변환
train_x = train_x.to_numpy()
train_y = train_y.to_numpy()
test = test.to_numpy()

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values='?', strategy='constant', fill_value=-1) #constant일 때, fill_value=-1로 채워줌 (영향 안 미치려고)
imputer.fit(train_x)
train_x = imputer.transform(train_x)
test = imputer.transform(test)
```
```py
print(np.unique(train_x['TotalCharges']))
(train_x['TotalCharges'] == ' ').sum() #공백이 몇개나 있는지 확인
drop_idx = train_x[train_x['TotalCharges'] == ' '].index #인덱스값 담아주기

train_x = train_x.drop(drop_idx, axis=0) # row를 드랍할것 이므로 axis=0 (디폴트)
train_y = train_y.drop(drop_idx, axis=0)
train_x['TotalCharges'] = train_x['TotalCharges'].astype('float64')

```

### 8. NaN값 처리

---

# <모델 - Auto ML>
```py
# install autoskearn
!apt-get install build-essential swig
!pip install auto-sklearn

!apt-get remove swig
!apt-get install swig3.0 build-essential -y
!ln -s /usr/bin/swig3.0 /usr/bin/swig
!pip install --upgrade setuptools
```
### 1. Auto ML (분류)
```py
import autosklearn.classification
from joblib import dump, load
automl = autosklearn.classification.AutoSklearnClassifier(
                                                           time_left_for_this_task=120,
                                                           per_run_time_limit=1000,    #수정가능
                                                           include_estimators=["k_nearest_neighbors",  "random_forest", 'decision_tree', 
                                                                                'libsvm_svc', 'adaboost', 'gradient_boosting', 'lda', 'qda'],
                                                            n_jobs=-1, ensemble_size = 3, seed = 1   #수정가능
                                                          )
automl.fit(train_x, train_y)
pred = automl.predict(test)
dump(automl, 'automl.joblib')
submit['target'] = pred
submit.to_csv('submit_automl.csv', index=False)
print(automl.show_models())
```

### 2. Auto ML (회귀)
```py
import autosklearn.regression
from joblib import dump, load
automl = autosklearn.regression.AutoSklearnRegressor(
                                                     time_left_for_this_task=120,
                                                     per_run_time_limit=1000,     #수정가능
                                                     include_estimators=["k_nearest_neighbors",  "random_forest", 'decision_tree', 
                                                                         'libsvm_svr', 'adaboost', 'gradient_boosting'],
                                                     n_jobs=-1, ensemble_size = 3, seed = 1   #수정가능
                                                    )
automl.fit(train_x, train_y)
pred = automl.predict(test)
dump(automl, 'automl.joblib')
submit['target'] = pred
submit.to_csv('submit_automl.csv', index=False)
print(automl.show_models())
```
---

# <모델 - 배운 것들>
### 1. KNN (분류 & 회귀)
```py
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=5, p=2)

from sklearn.neighbors import KNeighborsRegressor
model = KNeighborsRegressor(n_neighbors=5, weights= "distance") 
```
### 2. 로지스틱회귀 (분류)
```py
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(C=200, random_state=11) 
```
### 3. LDA & QDA (분류)
```py
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
model = LinearDiscriminantAnalysis(store_covariance=True, n_components=2)

from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
model = QuadraticDiscriminantAnalysis(store_covariance=True)
```
### 4. 의사결정나무 (분류 & 회귀)
```py
from sklearn.tree import DecisionTreeClassifier  
model = DecisionTreeClassifier()

from sklearn.tree import DecisionTreeRegressor
model = DecisionTreeRegressor()
```
### 5. SVM (분류 & 회귀)
```py
from sklearn.svm import SVC
model = SVC(kernel = ‘rbf’, C=10, gamma=“auto”)

from sklearn.svm import SVR
model = SVR(kernel = ‘rbf’, C=10, gamma=“auto”)
```

### 6. 앙상블 (voting)
```py
from sklearn.ensemble import VotingClassifier
logistic = LosisticRegression()
tree = DecisionTreeClassifier()
knn = KNeighborsClassifier()

Voting_estimators = [(‘logistic’, logistic), (‘tree’, tree), (‘knn’, knn)]
Voting = VotingClassifier(estimators = Voting_estimators, voting=‘soft’)

from sklearn.model_selection import GridSearchCV
Params = {‘logistic__C’ : [0.1, 1, 100], ‘tree__max_depth’:[1,3,5], ‘knn__n_neighbors’:[1,3,5]}
gcv = GridSearchCV(estimator=voting, param_grid = Params, cv=10, scoring=‘roc_auc’, iid=False)
gcv.fit(x_train, y_train)
print('final params', gcv.best_params_)
print('best score', gcv.best_score_)
```

### 7. 앙상블 (Bagging)
```py
from sklearn.ensemble import BaggingClassifier 

bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=None, criterion='entropy', random_state=1),
                            n_estimators=500,  
                            max_samples=1.0, 
                            max_features=1.0, 
                            bootstrap=True, 
                            bootstrap_features=False, 
                            n_jobs=1, 
                            random_state=1)
```
### 8. 앙상블 (Boosting)
```py
from sklearn.ensemble import AdaBoostClassifier 

adaboost = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1, criterion='entropy', random_state=1),
                              n_estimators=500,
                              learning_rate = 0.1, # 수정
                              random_state=1)
```

### 9. GBM, LightGBM
```py
from sklearn.ensemble import GradientBoostingClassifier
model = GradientBoostingClassifier(n_estimators=40)

from lightgbm import LGBMClassifier
model = LGBMClassifier(n_estimators=40)
```
---

# <모델 학습 및 예측값 저장>
```py
model.fit(train_x, train_y)
pred = model.pedict(test)
submit["target"] = pred
submit.to_csv('submit.csv', index=False)
```
---

# <기타>
### 1. 그리드 서치
```py
from sklearn.model_selection import GridSearchCV
model = svc(kernel=‘rbf’, class_weight=‘balanced’) 
param = {‘C’:[1,10,100,1000], ‘gamma’:[0.001,0.01,0.1]}
gcv = GridSearchCV(model, param, cv=10, n_jobs= -1)
gcv.fit()
print('final params', gcv.best_params_)
print('best score', gcv.best_score_)
```
### 2. KMeans 군집화
```py
from sklearn.cluster import KMeans
Kmeans = Kmeans(n_clusters=5, random_state=0)
Kmeans.fit(X)
Centers = Kmeans.cluster_centers_
```
### 4. 차원 축소 PCA
```py
from sklearn.decomposition import PCA
pca = PCA(n_components=150, svd_solver='randomized',whiten=True)
train_x_pca = pca.fit_transform(train_x)
test_pca = pca.transform(test)
```

### 5. 파이프라인
```py
from sklearn.pipeline import make_pipeline
pipeline = make_pipeline(StandardScaler(), PCA(), LogisticRegression())
pipeline.fit(train_x,train_y)
pred = pipeline.predict(test)

from sklearn.model_selection import cross_validate, GridSearchCV
scores = cross_validate(pipeline, train_x, train_y, cv = 10, return_train_sore=True)
gcv = GridSearchCV(pipeline, paraneters={}, scoring=‘accuracy’, cv = 10)
gcv.fit(x_train, y_train)
best_model = gcv.best_estimator_
pred = best_model.predict(test)
```


