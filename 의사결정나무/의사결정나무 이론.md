# 1. 의사결정나무 (Decision Tree)
## 1. 의사결정나무란?
- 학습 데이터를 분석하여 데이터에 내재되어 있는 패턴을 통해 새롭게 관측된 데이터를 예측 및 분류하는 모델
- 질문을 던져서 대상을 좁혀 나가는 '스무고개' 놀이와 비슷한 개념
- 목적(Y)과 자료(X)에 따라 적절한 **분리 기준**과 **정지 규칙**을 정하여 의사결정나무 생성

## 2. 의사결정나무의 장점
- 이해하기 쉽고, 적용하기 쉬움 : 나무 구조에 의한 표현 때문
- **의사결정과정에 대한 설명(해석) 가능** : 의료, 금융 분야 등에서 중요하게 사용
- 중요한 변수 선택에 유용
- 데이터의 통계적 가정이 필요 없음

## 3. 의사결정나무의 단점
- 좋은 모형을 만들기 위해 많은 데이터가 필요
- 모형을 만드는데 상대적으로 시간이 많이 소요
- 데이터의 변화에 민감 : 학습과 테스트 데이터의 도메인 갭이 작아야 함
- 선형 구조형 데이터 예측시 더 복잡 (붉은선 : 선형 회귀 결정 경계, 푸른선 : 의사결정나무 결정 경계)

![image](https://user-images.githubusercontent.com/70889699/120514934-c04d3580-c408-11eb-9437-149fd24af186.png)

## 4. 의사결정나무를 활용한 데이터 분석
### 1) 데이터 : 다변량 변수 사용
### 2) 모델 학습 (트리 구조 이용)
- 한번에 설명 변수 하나씩
- 데이터를 2개 이상의 부분집합으로 분할하여
- 데이터의 순도가 균일해지도록 재귀적 분할!
  - 분류 문제 : 끝 노드에 비슷한 클래스를 갖고 있는 관측 데이터끼지
  - 예측 문제 : 끝 노드에 비슷한 수치를 갖고 있는 관측 데이터 끼리
### 3) 추론 (판별)
  - 분류 : 끝 노드에서 가장 빈도가 높은 종속변수(y)를 새로운 데이터에 부여
  - 회귀 : 끝 노드의 종속변수(y)의 평균을 예측값으로 반환

## 5. 의사결정나무의 구분
### 1) 구분
- 분류 나무 : 목표 변수가 범주형 변수
- 회귀 나무 : 목표 변수가 수치형 변수

### 2) 재귀적 분할 알고리즘
- CART
- C4.5, C5.0
- CHAID

### 3) 불순도 알고리즘 (분할 기준)
- 지니 지수
- 엔트로피 지수, 정보 이익, 정보 이익률
- 카이제곱 통계량

# 2. 분류 나무 (Classification Tree)
- 목표 변수 : 범주형 변수
- 분류 알고리즘과 불순도 지표
  - CART : 지니 지수
  - C4.5 : 엔트로피 지수, 정보 이익, 정보 이익 비율
  - CHAID : 카이 제곱 통계량
- 분류 결과 : 소속 집단 판단, 경향성도 확률로 표현 가능

# 3. 회귀 나무 (Regression Tree)
- 목표 변수 : 수치형 변수
- 회귀 알고리즘과 불순도 지표
  - CART : F 통계량과 분산 감소량 (실제값과 예측값의 평균 차이가 작도록!)
- 회귀 결과 : 끝 마디 집단의 평균

# 4. 분할
## 1. 이진 분할 : CART
## 2. 다중 분할 : CHAID, C4.5, C5.0, etc.

# 5. 재귀적 분할 알고리즘 정리

![image](https://user-images.githubusercontent.com/70889699/120516111-f6d78000-c409-11eb-8246-a229cceeec6f.png)

## 1. CART (Classification & Regression Tree)
- Breiman 등이 개발
- 종류 : 분류 나무, 회귀 나무
- 분리 : 이진 분할
- 가지치기(교차 타당도) : 학습 데이터로 나무 생성, 검증용 데이터로 가지치기
- 불순도 알고리즘 : 지니 지수 (불확실성)는 낮아지는게 좋음

![image](https://user-images.githubusercontent.com/70889699/120516311-2d14ff80-c40a-11eb-98b3-6a5a39640d6c.png)

- 예제 : 10명의 고객을 대상으로 성별과 결혼 유무 중 어느 변수가 더 분류를 잘하는 변수인지 찾고, 분류 규칙을 찾고자 함

![image](https://user-images.githubusercontent.com/70889699/120516468-5afa4400-c40a-11eb-996c-9565a6ad992c.png)

## 2. C4.5, C5.0
- Quinlan 등이 개발
- 종류 : 분류 나무, 회귀 나무
- 분리 : 다중 분할
- 가지치기(교차 타당도) : 학습 데이터만 이용하여 나무 성장 및 가지치기 수행
- 불순도 알고리즘 : 엔트로피 지수(불확실성), 정보 이론, 정보 이득률
- 정보 이론 -> 엔트로피
  - log로 계산하는 이유는 bit 수로 정보를 계산하기 때문
  - -log로 계산하는 이유는 log(1/2) = -1 이기 때문에 +로 전환 필요

- 정보 이익(IG : Information Gain) : 정보의 가치 높아야 좋음
  - IG = E(before) - E(after)

![image](https://user-images.githubusercontent.com/70889699/120516870-cd6b2400-c40a-11eb-91ef-558c14668659.png)

- 정보 이득율(Information Gain Ratio)
- C4.5에서는 정보 이득율 추가 도입
- 가지수가 많을수록 IG가 높아지는 경향을 보임
- 단점 보안 위해 IV(Intrinsic Value) 도입하여 정보 이득율을 정규화 : 가지가 많으면 감점

![image](https://user-images.githubusercontent.com/70889699/120517077-03a8a380-c40b-11eb-9da4-0b8d5156ba67.png)

![image](https://user-images.githubusercontent.com/70889699/120517110-0acfb180-c40b-11eb-93dc-11ec641846f8.png)

# 6. 과적합 방지

## 1. 끝 없는 분할의 단점 : 과적합(오버피팅)

## 2. 과적합 피하는 법 : 나무 성장 중단

## 3. 모델 학습의 목적
- 잘못된 학습 : 학습용 데이터에서는 높은 성과 -> 평가용 데이터에서는 낮은 성과
- 올바른 학습 : 현재 데이터의 설명 -> 미래 데이터 예측

## 4. 성장 멈추기 (Stop Condition)
- 트리 모델의 깊이 파라미터로 설정
- 트리 모델을 성장시키면서 특정 조건에서 성장을 중단
- 노드 내의 최소 관측치의 수
- 불순도 최소 감소량
  - CHAID에서 사용
  - 가지치기 사용하지 않고 종료

## 5. 가지치기(Pruning)
- 완전 모형 생성 후 가지치기
- 데이터를 버리는 개념이 아니고 '합치는' 개념
- 트리 모델 생성 후 필요 없는 가지 제거
- 성장 멈추기 보다 성능 우수
- 가지치기 비용함수를 최소로 하는 분기를 찾음

# 7. 회귀 나무
- 입력 데이터(변수 값)의 결과 예측 : 데이터가 도달한 끝 노드 데이터들의 평균으로 결정
- 불순도 측정 방법
  - 제곱 오차 합(SSE)
  - 오차 = 실제값-예측값
- 성능 평가 방법
  - 예측모델 평가 방법 : RMSE

# 8. 앙상블
- 여러 모델을 함께 사용하자!
- 설명보다는 예측이 중요할 경우에 사용
- 랜덤 포레스트
  - 부트스트랩 사용 : 데이터로부터 복원 추출하여 여러 샘플 추출
  - 숲 생성 : 무작위로 예측 변수 선택하여 모델 구축
  - 앙상블 결과 결합 : 분류 문제 -> 투표, 예측 문제 -> 평균화
  - 비록 나무 구조이지만, 숲이 되면서 해석 가능한 모델의 장점은 사라짐
  - 그러나 결과 분석을 통해 설명 변수 중 중요한 변수 판별 가능


















