# 1. 차원 축소
## 1. 차원 축소 개요
### 1) 데이터 분석 과정
: 차원 축소는 '전처리'에서 주로 사용된다!
![image](https://user-images.githubusercontent.com/70889699/120487285-ca633a00-c3f0-11eb-8806-bb691a1060da.png)

### 2) 차원 축소의 필요성
- 고차원 데이터의 증가
- 차원의 저주
  - 차원이 증가할 수록 동일 정보량을 표현하기 위해 필요한 데이터의 수는 지수적으로 증가한다.
  - 학습 데이터의 수가 차원의 수보다 적어지면 모델의 성능이 저하
  - 데이터 차원이 증가할 수록 개별 차원 내 학습할 데이터 수가 적어지는(sparse) 현상
  - 무조건 변수의 수가 증가한다고 차원의 저주가 걸리는 것은 아니며, **데이터의 수보다 변수의 수가 많아지면 발생**
  - 일반적으로 instrinsic dimension은 original dimension 보다 상대적으로 작음

- 차원의 저주 요약
  - 차원이 높을 수록 발생하는 문제
  - 데이터에 포함될 노이즈의 비율도 높아짐 -> 성능 저하
  - 모델 학습과 추론의 계산 복잡도가 높아짐
  - 동일한 성능을 얻기 위해 더 많은 데이터가 필요함
  - 모든 변수가 서로 독립일 경우(이론적으로)에는 차원의 증가가 모델 성능을 향상 시키지만, 대부분 모든 변수는 서로 상관관계가 있음

- 차원의 저주 해결법
  - 도메인 지식을 이용하여 중요한 특성만 사용
  - 목적함수에 Regularization term 추가
  - **전처리에 차원 축소 기술 사용**

### 3) 차원 축소 목적 
: 모델의 성능을 최대로 해주는 변수의 일부 셋을 찾는 것

### 4) 차원 축소 효과
  - 변수간 상관 관계(correlations) 제거
  - 단순한 후처리(post-processing)
  - 적절한 정보를 유지하면서 중복되거나 불필요한 변수 제거
  - 시각화 가능

### 5) 지도학습 기반 차원 축소
: 학습 결과가 피드백 되어 Feature Selection 반복

![image](https://user-images.githubusercontent.com/70889699/120488770-0ba81980-c3f2-11eb-8d9b-61dcb8ebcb09.png)

### 6) 비지도 학습 기반 차원 축소
: 피드백을 통한 Feature Selection 반복 없음

![image](https://user-images.githubusercontent.com/70889699/120488890-237f9d80-c3f2-11eb-8dcd-291ad07d28c8.png)

### 7) 차원 축소 방법
- 변수/피쳐 **선택**(Feature Selection) : 유의미한 변수만 선택
  - 장점 : 선택한 변수 해석 용이
  - 단점 : 변수간 상관관계 고려의 어려움
- 변수/피쳐 **추출**(Feature Extraction) : 예측 변수의 변환을 통해 새로운 변수 추출
  - 장점 : 변수간 상관관계 고려, 변수의 개수를 많이 줄일 수 있음
  - 단점 : 추출된 변수의 해석이 어려움

## 2. 주성분 분석 (PCA)
### 1) 주성분 분석의 목적
- 차원을 줄이는 비지도 학습 방법 중 한가지
- 사영 후 원 데이터의 분산을 최대한 보존할 수 있는기저를 찾아 차원을 줄이는 방법

### 2) 주성분 분석
- 데이터를 사영 시킬 경우 손실되는 정보의 양이 적은 쪽의 기저(축)를 선택

![image](https://user-images.githubusercontent.com/70889699/120489562-b15b8880-c3f2-11eb-9b86-55dfc9a2d5b5.png)

### 3) 주성분 분석의 수리적 배경
**(1) 선형 결합 : 데이터(X) 사영 변환 후(Z)에도 분산이 보존하는 기저(a)를 찾는 것**

![image](https://user-images.githubusercontent.com/70889699/120489776-dcde7300-c3f2-11eb-8e4e-5f2c00c1eb62.png)

**(2) 공분산(Covariance) : 변수의 상관 정도**
![image](https://user-images.githubusercontent.com/70889699/120490006-04354000-c3f3-11eb-82a7-7dcf2adea276.png)

**(3) 사영(Projection)**

![image](https://user-images.githubusercontent.com/70889699/120490074-13b48900-c3f3-11eb-8efe-2b34e5ee0dc9.png)
- 행렬 A가 Non-singular 하다면, d개의 고유값과 고유벡터가 존재함
- 고유벡터는 서로 직교함 (PC1과 PC2의 주축은 서로 직교한다)

**(4) 고유값(eigenvalue)과 고유벡터(eigenvector)**

![image](https://user-images.githubusercontent.com/70889699/120490218-2fb82a80-c3f3-11eb-8bd7-bc1ade168b42.png)

**(5) 벡터에 행렬을 곱하는 것은 선형 변환의 의미를 가짐**
- 고유벡터는 변환에 의해 방향 변화가 발생하지 않음
- 고유벡터의 크기 변화는 람다 만큼

### 4) 주성분 분석 알고리즘
**(1) STEP1 : 데이터 센터링 (Data Centering)**
: 데이터의 평균을 0으로 변경

![image](https://user-images.githubusercontent.com/70889699/120490872-bbca5200-c3f3-11eb-8dcf-46d749252c87.png)

**(2) STEP2 : 최적화 문제 정의**

![image](https://user-images.githubusercontent.com/70889699/120490962-d00e4f00-c3f3-11eb-8049-963d0929e929.png)


**(3) STEP3 : 최적화 문제 솔루션**
: 라그랑지 멀티플라이어 적용

![image](https://user-images.githubusercontent.com/70889699/120491098-ecaa8700-c3f3-11eb-8a74-f884ade9bb38.png)

**(4) STEP4 : 주축 정렬**

![image](https://user-images.githubusercontent.com/70889699/120491253-0d72dc80-c3f4-11eb-95c2-0a336f230c50.png)

**(5) STEP5 : PCA로 변환된 데이터**

![image](https://user-images.githubusercontent.com/70889699/120491344-211e4300-c3f4-11eb-8037-7b880f351205.png)

**(6) STEP6 : 원데이터로 복원**
: 2D 데이터 -> 1D PCA -> 2D 데이터 복원

![image](https://user-images.githubusercontent.com/70889699/120491473-398e5d80-c3f4-11eb-9d09-f4962a77c909.png)

### 5) 주성분 분석 이슈
**- 몇 개의 주성분을 사용해야 할까?**
  - 고유값 감소율이 유의미하게 낮아지는 Elbow Point에 해당하는 주성분 선택
  - 일정 수준 이상의 분산비를 보존하는 최소의 주성분 선택 (보통 70% 이상)
  
### 5) 주성분 분석 이슈
- 한계점 1 : 데이터 분포가 가우시안이 아니거나, 다중 가우시안 자료에 대해서는 적용이 어려움
- 한계점 2 : 분류 문제를 위해 디자인 되지 않음, 즉 분류 성능 향상을 보장하지 못함

![image](https://user-images.githubusercontent.com/70889699/120492149-c802df00-c3f4-11eb-965b-fa6a4e159a6a.png)


## 3. 랜덤 PCA(Randomized PCA)
- 자료의 크기 또는 특성변수의 크기가 매우 크면 주성분 W를구하기 위한 SVD계산이 불가능하거나 시간이 많이 소요됨
- 이런 경우 랜덤 PCA가 유용
- 랜덤 PCA는 QR분해를 이용해 행렬의 SVD를 수행

## 4. 커널 PCA(Kernelized PCA)
- PCA는 선형 변환이고, 커널 PCA는 비선형 변환
- SVM의 커널 트릭을 PCA에서도 사용
- 특성 변수 x를 비선형 h(x)로 변환 후, 이에 대해 PCA를 하여 차원을 축소

![image](https://user-images.githubusercontent.com/70889699/120492812-5bd4ab00-c3f5-11eb-9324-5fd51202ff9a.png)






